



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../docs/pywarm-logo-small-dark.gif">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.4.2">
    
    
      
        <title>Functional - pywarm</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.30686662.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#ff7043">
      
    
    
      <script src="../../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="deep-orange" data-md-color-accent="pink">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github">
    <g transform="scale(18.0)">
        <path fill="currentColor" d="m12,0.297c-6.63,0 -12,5.373 -12,12c0,5.303 3.438,9.8 8.205,11.385c0.6,0.113 0.82,-0.258 0.82,-0.577c0,-0.285 -0.01,-1.04 -0.015,-2.04c-3.338,0.724 -4.042,-1.61 -4.042,-1.61c-0.546,-1.385 -1.335,-1.755 -1.335,-1.755c-1.087,-0.744 0.084,-0.729 0.084,-0.729c1.205,0.084 1.838,1.236 1.838,1.236c1.07,1.835 2.809,1.305 3.495,0.998c0.108,-0.776 0.417,-1.305 0.76,-1.605c-2.665,-0.3 -5.466,-1.332 -5.466,-5.93c0,-1.31 0.465,-2.38 1.235,-3.22c-0.135,-0.303 -0.54,-1.523 0.105,-3.176c0,0 1.005,-0.322 3.3,1.23c0.96,-0.267 1.98,-0.399 3,-0.405c1.02,0.006 2.04,0.138 3,0.405c2.28,-1.552 3.285,-1.23 3.285,-1.23c0.645,1.653 0.24,2.873 0.12,3.176c0.765,0.84 1.23,1.91 1.23,3.22c0,4.61 -2.805,5.625 -5.475,5.92c0.42,0.36 0.81,1.096 0.81,2.22c0,1.606 -0.015,2.896 -0.015,3.286c0,0.315 0.21,0.69 0.825,0.57c4.801,-1.574 8.236,-6.074 8.236,-11.369c0,-6.627 -5.373,-12 -12,-12"/>
    </g>
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#module-warmfunctional" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../.." title="pywarm" class="md-header-nav__button md-logo">
          
            <img src="../../../docs/pywarm-logo-small-light.gif" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              pywarm
            </span>
            <span class="md-header-nav__topic">
              
                Functional
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/blue-season/pywarm.git/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    blue-season/pywarm
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../.." title="pywarm" class="md-nav__button md-logo">
      
        <img src="../../../docs/pywarm-logo-small-light.gif" width="48" height="48">
      
    </a>
    pywarm
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/blue-season/pywarm.git/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    blue-season/pywarm
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../CONTRIBUTING/" title="Contributing" class="md-nav__link">
      Contributing
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../docs/example/" title="Example" class="md-nav__link">
      Example
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../docs/tutorial/" title="Tutorial" class="md-nav__link">
      Tutorial
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5" checked>
    
    <label class="md-nav__link" for="nav-5">
      Reference
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Reference
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1" type="checkbox" id="nav-5-1" checked>
    
    <label class="md-nav__link" for="nav-5-1">
      Warm
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-5-1">
        Warm
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../engine/" title="Engine" class="md-nav__link">
      Engine
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Functional
      </label>
    
    <a href="./" title="Functional" class="md-nav__link md-nav__link--active">
      Functional
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batch_norm" class="md-nav__link">
    batch_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conv" class="md-nav__link">
    conv
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    dropout
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding" class="md-nav__link">
    embedding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gru" class="md-nav__link">
    gru
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#identity" class="md-nav__link">
    identity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#layer_norm" class="md-nav__link">
    layer_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear" class="md-nav__link">
    linear
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstm" class="md-nav__link">
    lstm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    transformer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module/" title="Module" class="md-nav__link">
      Module
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../util/" title="Util" class="md-nav__link">
      Util
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batch_norm" class="md-nav__link">
    batch_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conv" class="md-nav__link">
    conv
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    dropout
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding" class="md-nav__link">
    embedding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gru" class="md-nav__link">
    gru
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#identity" class="md-nav__link">
    identity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#layer_norm" class="md-nav__link">
    layer_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear" class="md-nav__link">
    linear
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstm" class="md-nav__link">
    lstm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    transformer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/blue-season/pywarm.git/edit/master/docs/reference/warm/functional.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="module-warmfunctional">Module warm.functional</h1>
<p>Wraps around various torch.nn Modules to fit into a functional interface.</p>
<h2 id="functions">Functions</h2>
<hr />
<h3 id="batch_norm">batch_norm</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="n">x</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Batch Normalization layer.</p>
<ul>
<li><code>x: Tensor</code>; 2d or more, with shapes <code>(Batch, Channel, *)</code> where <code>*</code> means any number of additional dimensions.</li>
<li><code>**kw: dict</code>; Any additional KWargs are passed down to <code>torch.nn.BatchNormNd</code>, where N can be 1, 2 or 3.
    as well as <code>warm.engine.forward</code>. Refer to their docs for details. Some of the additional BatchNorm arguments:
    <code>eps, momentum, affine, track_running_stats</code>.</li>
<li><code>return: Tensor</code>; Same shape as input  <code>x</code>.</li>
</ul>
<hr />
<h3 id="conv">conv</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="n">x</span><span class="p">,</span>
  <span class="n">size</span><span class="p">,</span>
  <span class="n">kernel</span><span class="p">,</span>
  <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Convolution layer.</p>
<ul>
<li><code>x: Tensor</code>; With shape <code>(Batch, Channel, *)</code> where <code>*</code> Can be 1d or 2d or 3d.
    If 3d, shapes are <code>(Batch, Channel, Length)</code>.
    If 4d, shapes are <code>(Batch, Channel, Height, Width)</code>.
    If 5d, shapes are <code>(Batch, Channel, Depth, Height, Width)</code>.</li>
<li><code>size: int</code>; Size of hidden filters, and size of the output channel.</li>
<li><code>kernel: int or tuple</code>; Size of the convolution kernel.</li>
<li><code>init_weight: None or str or callable</code>; Initialization specification for the weight tensor.
    If a <code>str</code>, should be one of the nonlinearity functions contained in <code>torch.nn.init</code>.
    If a <code>callable</code>, it will be applied to <code>x</code> directly, i.e. <code>spec(x)</code>. If a 2-<code>tuple</code>,
    it must be of format <code>(callable, kwargs)</code>, i.e. <code>callable(x, **kwargs)</code>.
    Default: <code>None</code>, and the weight tensor is initialized using <code>torch.nn.ConvNd</code>s default scheme.</li>
<li><code>init_bias: None or str or callable</code>; Same as <code>init_weight</code>, but for the bias tensor.</li>
<li><code>bias: bool</code>; If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>.</li>
<li><code>**kw:dict</code>; Any additional KWargs are passed down to <code>torch.nn.ConvNd</code>, where N can be 1, 2 or 3.
    as well as <code>warm.engine.forward</code>. Refer to their docs for details. Some of the additional ConvNd arguments:
    <code>stride, padding, dilation, groups</code>.</li>
<li><code>return: Tensor</code>; With shape <code>(Batch, Size, *)</code> where <code>*</code> can be 1d, 2d, 3d that depends on <code>x</code>.</li>
</ul>
<hr />
<h3 id="dropout">dropout</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="n">x</span><span class="p">,</span>
  <span class="n">rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
  <span class="n">by_channel</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Dropout layer.</p>
<p>During training, randomly zeros part of input tensor <code>x</code>, at probability <code>rate</code>.</p>
<ul>
<li><code>x: Tensor</code>; Can be of any shape if <code>by_channel</code> is false, or 2d and up if <code>by_channel</code> is true.</li>
<li><code>rate: float</code>; The probability of dropout. Default 0.5.</li>
<li><code>by_channel: bool</code>; If true, will dropout entire channels (all <code>'D'</code> dimensions will be 0 if x is <code>'BCD'</code>).
    <code>by_channel</code> true requires <code>x</code> to be 2d or more.</li>
<li><code>inplace: bool</code>; If true, the operation will be in-place and the input <code>x</code> will be altered.</li>
<li><code>return: Tensor</code>; Same shape as <code>x</code>.</li>
</ul>
<hr />
<h3 id="embedding">embedding</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="n">x</span><span class="p">,</span>
  <span class="n">size</span><span class="p">,</span>
  <span class="n">vocabulary</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Embedding layer.</p>
<p>The input is usually a list of indices (integers), and the output is a dense matrix which
maps indices to dense vectors. Thus the output will have 1 more dimension than the input.</p>
<p><strong>Note</strong>: The output of this function is always one more dimension than the input. For input with shape <code>(*)</code>,
The output will be <code>(*, size)</code>. Any shape specifications in the KWargs are ignored. </p>
<ul>
<li><code>x: Tensor</code>; Contains indices into the vocabulary. Will be converted to <code>LongTensor</code> of integers.
    Can be of any shape.</li>
<li><code>size: int</code>; The size of embedding vector.</li>
<li><code>vocabulary: int or None</code>; The size of vocabulary of embedding, or max number of unique indices in <code>x</code>.
    By default it is set to <code>max(x)-min(x)+1</code>.</li>
<li><code>**kw: dict</code>; Any additional KWargs are passed down to <code>torch.nn.LayerNorm</code>, as well as <code>warm.engine.forward</code>.</li>
<li><code>return: Tensor</code>; With the embedded dim appended to the shape of x.
    Thus with shape <code>(*, Size)</code>, where <code>*</code> is the shape of <code>x</code>.</li>
</ul>
<hr />
<h3 id="gru">gru</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="o">*</span><span class="n">arg</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Gated Recurrent Unit layer.</p>
<ul>
<li><code>x: Tensor</code>; 3d, with shapes <code>(Batch, Channel, Length)</code>.</li>
<li><code>size: int</code>; Size of hidden features, and size of the output channel.</li>
<li><code>init_weight_hh: None or str or callable</code>; Initialization specification for the hidden-hidden weight tensor.
    If a <code>str</code>, should be one of the nonlinearity functions contained in <code>torch.nn.init</code>.
    If a <code>callable</code>, it will be applied to <code>x</code> directly, i.e. <code>spec(x)</code>. If a 2-<code>tuple</code>,
    it must be of format <code>(callable, kwargs)</code>, i.e. <code>callable(x, **kwargs)</code>.
    Default: <code>'orthogonal_'</code>.</li>
<li><code>init_weight_ih: None or str or callable</code>; Initialization specification for the input-hidden weight tensor.
    Default: <code>None</code>, and the weight tensor is initialized using <code>torch.nn.GRU</code>s default scheme.</li>
<li><code>init_bias_hh: None or str or callable</code>; Initialization specification for the hidden-hidden bias tensor.
    Default: <code>None</code>, and the weight tensor is initialized using <code>torch.nn.GRU</code>s default scheme.</li>
<li><code>init_bias_ih: None or str or callable</code>; Initialization specification for the input-hidden bias tensor.
    Default: <code>None</code>, and the weight tensor is initialized using <code>torch.nn.GRU</code>s default scheme.</li>
<li><code>bias: bool</code>; If <code>False</code>, then the layer does not use <code>bias_ih</code> and <code>bias_hh</code>. Default: <code>True</code>.</li>
<li><code>num_layers: int</code>; Number of the recurrent layers. Default: 1.</li>
<li><code>tuple_out: bool</code>; If <code>True</code>, the returned value will be a tuple <code>(out, (h_n, c_n))</code>. Default: False.</li>
<li><code>**kw: dict</code>; Any additional KWargs are passed down to <code>torch.nn.GRU</code>, as well as <code>warm.engine.forward</code>.
    Refer to their docs for details. Some of the additional GRU arguments: <code>dropout, bidirectional, batch_first</code>.</li>
<li><code>return: Tensor or tuple</code>; If <code>tuple_out</code> set to true, will return <code>(out, (h_n, c_n)</code>, otherwise just <code>out</code>.
    <code>out</code> has shape <code>(Batch, Size, Length*Directions)</code>,
        where Directions = 2 if <code>bidirectional</code> else 1.
    <code>h_n</code> is the hidden states with shape <code>(num_layers*Directions, Batch, Size)</code>.
    <code>c_n</code> is the cell states with shape <code>(num_layers*Directions, Batch, Size)</code>.</li>
</ul>
<hr />
<h3 id="identity">identity</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="n">x</span><span class="p">,</span>
  <span class="o">*</span><span class="n">arg</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Identity layer that returns the first input, ignores the rest arguments.</p>
<hr />
<h3 id="layer_norm">layer_norm</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="n">x</span><span class="p">,</span>
  <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Layer Normalization.</p>
<ul>
<li><code>x: Tensor</code>; Can be of any shape.</li>
<li><code>dim: int or list of int</code>; Dimensions to be normalized. Default: 1.</li>
<li><code>**kw: dict</code>; Any additional KWargs are passed down to <code>torch.nn.LayerNorm</code>, as well as <code>warm.engine.forward</code>.</li>
<li><code>return: Tensor</code>; Same shape as <code>x</code>.</li>
</ul>
<hr />
<h3 id="linear">linear</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="n">x</span><span class="p">,</span>
  <span class="n">size</span><span class="p">,</span>
  <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Linear transformation layer.</p>
<ul>
<li><code>x: Tensor</code>; 2d or more, with shapes <code>(Batch, Channel, *)</code> where <code>*</code> means any number of additional dimensions.</li>
<li><code>size: int</code>; Size of hidden features, and size of the output channel.</li>
<li><code>init_weight: None or str or callable</code>; Initialization specification for the weight tensor.
    If a <code>str</code>, should be one of the nonlinearity functions contained in <code>torch.nn.init</code>.
    If a <code>callable</code>, it will be applied to <code>x</code> directly, i.e. <code>spec(x)</code>. If a 2-<code>tuple</code>,
    it must be of format <code>(callable, kwargs)</code>, i.e. <code>callable(x, **kwargs)</code>.
    Default: <code>None</code>, and the weight tensor is initialized using <code>torch.nn.Linear</code>s default scheme.</li>
<li><code>init_bias: None or str or callable</code>; Same as <code>init_weight</code>, but for the bias tensor.</li>
<li><code>bias: bool</code>; If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>.</li>
<li><code>**kw:dict</code>; Any additional KWargs are passed down to <code>warm.engine.forward</code>. Refer to its docs for details.</li>
<li><code>return: Tensor</code>; With shape <code>(Batch, Size, *)</code> where <code>*</code> can be 1d, 2d, 3d that depends on <code>x</code>.</li>
</ul>
<hr />
<h3 id="lstm">lstm</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="n">x</span><span class="p">,</span>
  <span class="n">size</span><span class="p">,</span>
  <span class="n">init_weight_hh</span><span class="o">=</span><span class="s1">&#39;orthogonal_&#39;</span><span class="p">,</span>
  <span class="n">init_weight_ih</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">init_bias_hh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">init_bias_ih</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
  <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Long Short Term Memory layer.</p>
<ul>
<li><code>x: Tensor</code>; 3d, with shapes <code>(Batch, Channel, Length)</code>.</li>
<li><code>size: int</code>; Size of hidden features, and size of the output channel.</li>
<li><code>init_weight_hh: None or str or callable</code>; Initialization specification for the hidden-hidden weight tensor.
    If a <code>str</code>, should be one of the nonlinearity functions contained in <code>torch.nn.init</code>.
    If a <code>callable</code>, it will be applied to <code>x</code> directly, i.e. <code>spec(x)</code>. If a 2-<code>tuple</code>,
    it must be of format <code>(callable, kwargs)</code>, i.e. <code>callable(x, **kwargs)</code>.
    Default: <code>'orthogonal_'</code>.</li>
<li><code>init_weight_ih: None or str or callable</code>; Initialization specification for the input-hidden weight tensor.
    Default: <code>None</code>, and the weight tensor is initialized using <code>torch.nn.LSTM</code>s default scheme.</li>
<li><code>init_bias_hh: None or str or callable</code>; Initialization specification for the hidden-hidden bias tensor.
    Default: <code>None</code>, and the weight tensor is initialized using <code>torch.nn.LSTM</code>s default scheme.</li>
<li><code>init_bias_ih: None or str or callable</code>; Initialization specification for the input-hidden bias tensor.
    Default: <code>None</code>, and the weight tensor is initialized using <code>torch.nn.LSTM</code>s default scheme.</li>
<li><code>bias: bool</code>; If <code>False</code>, then the layer does not use <code>bias_ih</code> and <code>bias_hh</code>. Default: <code>True</code>.</li>
<li><code>num_layers: int</code>; Number of the recurrent layers. Default: 1.</li>
<li><code>tuple_out: bool</code>; If <code>True</code>, the returned value will be a tuple <code>(out, (h_n, c_n))</code>. Default: False.</li>
<li><code>**kw: dict</code>; Any additional KWargs are passed down to <code>torch.nn.LSTM</code>, as well as <code>warm.engine.forward</code>.
    Refer to their docs for details. Some of the additional LSTM arguments: <code>dropout, bidirectional, batch_first</code>.</li>
<li><code>return: Tensor or tuple</code>; If <code>tuple_out</code> set to true, will return <code>(out, (h_n, c_n)</code>, otherwise just <code>out</code>.
    <code>out</code> has shape <code>(Batch, Size, Length*Directions)</code>,
        where Directions = 2 if <code>bidirectional</code> else 1.
    <code>h_n</code> is the hidden states with shape <code>(num_layers*Directions, Batch, Size)</code>.
    <code>c_n</code> is the cell states with shape <code>(num_layers*Directions, Batch, Size)</code>.</li>
</ul>
<hr />
<h3 id="transformer">transformer</h3>
<p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="err">:</span>
    <span class="n">x</span><span class="p">,</span>
  <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">num_encoder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
  <span class="n">num_decoder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
  <span class="n">num_head</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
  <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
  <span class="n">in_shape</span><span class="o">=</span><span class="s1">&#39;BCD&#39;</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kw</span> 
</pre></div>
Transformer layer.</p>
<p>This layer covers functionality of <code>Transformer</code>, <code>TransformerEncoder</code>, and <code>TransformerDecoder</code>.
See <a href="https://pytorch.org/docs/stable/nn.html#transformer"><code>torch.nn.Transformer</code></a> for more details.</p>
<ul>
<li><code>x: Tensor</code>; The source sequence, with shape <code>(Batch, Channel, LengthX)</code>.
    <code>Channel</code> is usually from embedding.</li>
<li><code>y: None or Tensor</code>; The target sequence. Also with shape <code>(Batch, Channel, LengthY)</code>.
    If not present, default to equal <code>x</code>.</li>
<li><code>num_encoder: int</code>; Number of encoder layers. Set to 0 to disable encoder and use only decoder. Default 6.</li>
<li><code>num_decoder: int</code>; Number of decoder layers. Set to 0 to disable decoder and use only encoder. Default 6.</li>
<li><code>num_head: int</code>; Number of heads for multi-headed attention. Default 8.</li>
<li><code>mask: None or dict</code>; Keys are among: <code>src_mask</code>, <code>tgt_mask</code>, <code>memory_mask</code>,
    <code>src_key_padding_mask</code>, <code>tgt_key_padding_mask</code>, <code>memory_key_padding_mask</code>.
    See the <code>forward</code> method of <code>torch.nn.Transformer</code> for details.</li>
<li><code>causal: bool</code>; Default false. if true, will add causal masks to source and target, so that
    current value only depends on the past, not the future, in the sequences.</li>
<li><code>**kw: dict</code>; Any additional KWargs are passed down to <code>torch.nn.Transformer</code>, as well as <code>warm.engine.forward</code>.</li>
<li><code>return: Tensor</code>; Same shape as <code>y</code>, if <code>num_decoder</code> &gt; 0. Otherwise same shape as <code>x</code>.</li>
</ul>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../engine/" title="Engine" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Engine
              </span>
            </div>
          </a>
        
        
          <a href="../" title="Index" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Index
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.c648116f.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
    
  </body>
</html>